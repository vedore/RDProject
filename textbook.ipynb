{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple ontology alignment framework in python: using lexical similarity and LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Transformers Pipeline options\n",
    "\n",
    "pipeline = [\n",
    "    'audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages:\n",
    "\n",
    "- pip3 install torch torchvision torchaudio\n",
    "- pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "- pip3 install transformers\n",
    "- pip3 install ipywidgets\n",
    "- pip3 install rdflib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding, labbeling\n",
    "\n",
    "task conference, anatonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import os\n",
    "import torch\n",
    "import rdflib\n",
    "from rdflib.namespace import RDFS\n",
    "from collections import Counter\n",
    "from transformers import BertModel, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = os.getcwd() + \"\\\\models\"\n",
    "tokens_path = os.getcwd() + \"\\\\tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c:\\\\Users\\\\tomas\\\\Documents\\\\faculdade\\\\mestrado\\\\2ºsemestre\\\\Redes_de_conhecimento\\\\RDProject\\\\tokens\\\\tokenizer_config.json',\n",
       " 'c:\\\\Users\\\\tomas\\\\Documents\\\\faculdade\\\\mestrado\\\\2ºsemestre\\\\Redes_de_conhecimento\\\\RDProject\\\\tokens\\\\special_tokens_map.json',\n",
       " 'c:\\\\Users\\\\tomas\\\\Documents\\\\faculdade\\\\mestrado\\\\2ºsemestre\\\\Redes_de_conhecimento\\\\RDProject\\\\tokens\\\\vocab.txt',\n",
       " 'c:\\\\Users\\\\tomas\\\\Documents\\\\faculdade\\\\mestrado\\\\2ºsemestre\\\\Redes_de_conhecimento\\\\RDProject\\\\tokens\\\\added_tokens.json',\n",
       " 'c:\\\\Users\\\\tomas\\\\Documents\\\\faculdade\\\\mestrado\\\\2ºsemestre\\\\Redes_de_conhecimento\\\\RDProject\\\\tokens\\\\tokenizer.json')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bert_base_cased_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_base_cased_model.save_pretrained(models_path)\n",
    "\n",
    "# bert_sequence_classification = BertModel.from_pretrained(\"BertForSequenceClassification\")\n",
    "# bert_sequence_classification.save_pretrained(\"C:\\\\Faculdade\\\\Redes_De_Conhecimento\\\\RDProject\\\\models\")\n",
    "\n",
    "\n",
    "bert_base_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_base_tokenizer.save_pretrained(tokens_path)\n",
    "\n",
    "# bert_sequence_classification_tokenizer = AutoTokenizer.from_pretrained(\"BertForSequenceClassification\")\n",
    "# bert_sequence_classification_tokenizer.save_pretrained(\"C:\\\\Faculdade\\\\Redes_De_Conhecimento\\\\RDProject\\\\tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Ontologies\n",
    "def load_ontology(file_path):\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(file_path)\n",
    "    return g\n",
    "\n",
    "ontology1 = load_ontology(os.getcwd()+\"\\\\anatomy-dataset\\\\anatomy-dataset\\\\human.owl\")\n",
    "ontology2 = load_ontology(os.getcwd()+\"\\\\anatomy-dataset\\\\anatomy-dataset\\\\mouse.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract labels from ontology\n",
    "def extract_labels(ontology):\n",
    "    labels_dict = {}\n",
    "    owlClass = rdflib.namespace.OWL.Class\n",
    "    rdfType = rdflib.namespace.RDF.type\n",
    "\n",
    "    for s in ontology.subjects(predicate=rdfType, object=owlClass):\n",
    "        # Check if label is present locally\n",
    "        label = None\n",
    "        for o in ontology.objects(subject=s, predicate=RDFS.label):\n",
    "            if isinstance(o, rdflib.Literal):\n",
    "                label = str(o)\n",
    "                break\n",
    "        if label:\n",
    "            class_uri = str(s)\n",
    "            if class_uri in labels_dict:\n",
    "                labels_dict[class_uri].append(label)\n",
    "            else:\n",
    "                labels_dict[class_uri] = [label]\n",
    "        else:\n",
    "            # Load external ontology to find label\n",
    "            for o in ontology.objects(subject=s, predicate=rdflib.namespace.RDFS.seeAlso):\n",
    "                g_external = rdflib.Graph()\n",
    "                g_external.parse(str(o))\n",
    "                for o_external in g_external.objects(subject=s, predicate=RDFS.label):\n",
    "                    if isinstance(o_external, rdflib.Literal):\n",
    "                        label_external = str(o_external)\n",
    "                        class_uri = str(s)\n",
    "                        if class_uri in labels_dict:\n",
    "                            labels_dict[class_uri].append(label_external)\n",
    "                        else:\n",
    "                            labels_dict[class_uri] = [label_external]\n",
    "                        break  # Exit loop after finding the label\n",
    "                    break  # Exit loop after finding the label\n",
    "    return labels_dict\n",
    "\n",
    "# Extract labels for ontology1 and ontology2\n",
    "labels_dict1 = extract_labels(ontology1)\n",
    "labels_dict2 = extract_labels(ontology2)\n",
    "\n",
    "# Function to select primary label for each class\n",
    "def select_primary_label(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    primary_label = label_counts.most_common(1)[0][0] if label_counts else None\n",
    "    return primary_label\n",
    "\n",
    "# Apply label selection to each ontology\n",
    "primary_labels_dict1 = {class_uri: select_primary_label(labels) for class_uri, labels in labels_dict1.items()}\n",
    "primary_labels_dict2 = {class_uri: select_primary_label(labels) for class_uri, labels in labels_dict2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate BERT embeddings for a list of labels\n",
    "def generate_bert_embeddings(labels, model, tokenizer):\n",
    "    labels_list = list(labels.values())\n",
    "\n",
    "    # Tokenize the labels\n",
    "    tokenized_labels = tokenizer(labels_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_labels)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over tokens\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Example usage with reduced batch size:\n",
    "embeddings_ontology1 = generate_bert_embeddings(primary_labels_dict1, bert_base_cased_model, bert_base_tokenizer)\n",
    "embeddings_ontology2 = generate_bert_embeddings(primary_labels_dict2, bert_base_cased_model, bert_base_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
